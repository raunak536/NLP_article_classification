{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('flipitnews-data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2225, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Technology</td>\n",
       "      <td>tv future in the hands of viewers with home th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Business</td>\n",
       "      <td>worldcom boss  left books alone  former worldc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sports</td>\n",
       "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sports</td>\n",
       "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Entertainment</td>\n",
       "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Category                                            Article\n",
       "0     Technology  tv future in the hands of viewers with home th...\n",
       "1       Business  worldcom boss  left books alone  former worldc...\n",
       "2         Sports  tigers wary of farrell  gamble  leicester say ...\n",
       "3         Sports  yeading face newcastle in fa cup premiership s...\n",
       "4  Entertainment  ocean s twelve raids box office ocean s twelve..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sports           23.0\n",
       "Business         23.0\n",
       "Politics         19.0\n",
       "Technology       18.0\n",
       "Entertainment    17.0\n",
       "Name: Category, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(df['Category'].value_counts(1, dropna=False) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     2225.00000\n",
       "mean      2262.93618\n",
       "std       1364.10253\n",
       "min        501.00000\n",
       "25%       1446.00000\n",
       "50%       1965.00000\n",
       "75%       2802.00000\n",
       "max      25483.00000\n",
       "Name: Article, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Article'].str.len().describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(x):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    wnl = WordNetLemmatizer()\n",
    "\n",
    "    x = x.lower()\n",
    "    x = word_tokenize(x)\n",
    "    x = [wnl.lemmatize(word) for word in x if word not in stop_words if word.isalpha()]\n",
    "    \n",
    "    return \" \".join(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['CleanedArticle'] = df['Article'].apply(lambda x : pre_process(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Article</th>\n",
       "      <th>CleanedArticle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Technology</td>\n",
       "      <td>tv future in the hands of viewers with home th...</td>\n",
       "      <td>tv future hand viewer home theatre system plas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Business</td>\n",
       "      <td>worldcom boss  left books alone  former worldc...</td>\n",
       "      <td>worldcom bos left book alone former worldcom b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sports</td>\n",
       "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
       "      <td>tiger wary farrell gamble leicester say rushed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sports</td>\n",
       "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
       "      <td>yeading face newcastle fa cup premiership side...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Entertainment</td>\n",
       "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
       "      <td>ocean twelve raid box office ocean twelve crim...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Category                                            Article  \\\n",
       "0     Technology  tv future in the hands of viewers with home th...   \n",
       "1       Business  worldcom boss  left books alone  former worldc...   \n",
       "2         Sports  tigers wary of farrell  gamble  leicester say ...   \n",
       "3         Sports  yeading face newcastle in fa cup premiership s...   \n",
       "4  Entertainment  ocean s twelve raids box office ocean s twelve...   \n",
       "\n",
       "                                      CleanedArticle  \n",
       "0  tv future hand viewer home theatre system plas...  \n",
       "1  worldcom bos left book alone former worldcom b...  \n",
       "2  tiger wary farrell gamble leicester say rushed...  \n",
       "3  yeading face newcastle fa cup premiership side...  \n",
       "4  ocean twelve raid box office ocean twelve crim...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_enc = LabelEncoder()\n",
    "df['CategoryEncoded'] = label_enc.fit_transform(df['Category'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['CleanedArticle'], df['CategoryEncoded'], test_size=0.25, random_state=42, stratify=df['CategoryEncoded'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1668,), (557,))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(max_df=0.8, min_df=5, ngram_range=(1,2))\n",
    "tf_idf = TfidfVectorizer(max_df=0.8, min_df=5, ngram_range=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cv = pd.DataFrame(cv.fit_transform(X_train).todense())\n",
    "X_train_cv.columns = cv.get_feature_names()\n",
    "\n",
    "X_test_cv = pd.DataFrame(cv.transform(X_test).todense())\n",
    "X_test_cv.columns = cv.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1668, 10721), (557, 10721))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_cv.shape, X_test_cv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaa</th>\n",
       "      <th>aaron</th>\n",
       "      <th>abandoned</th>\n",
       "      <th>abandoning</th>\n",
       "      <th>abbott</th>\n",
       "      <th>abc</th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>able access</th>\n",
       "      <th>able get</th>\n",
       "      <th>...</th>\n",
       "      <th>yukos said</th>\n",
       "      <th>yushchenko</th>\n",
       "      <th>zealand</th>\n",
       "      <th>zero</th>\n",
       "      <th>zhang</th>\n",
       "      <th>zimbabwe</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zone</th>\n",
       "      <th>zurich</th>\n",
       "      <th>zurich premiership</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 10721 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   aaa  aaron  abandoned  abandoning  abbott  abc  ability  able  able access  \\\n",
       "0    0      0          0           0       0    0        0     0            0   \n",
       "1    0      0          0           0       0    0        0     0            0   \n",
       "2    0      0          0           0       0    0        0     0            0   \n",
       "3    0      0          0           0       0    0        0     0            0   \n",
       "4    0      0          0           0       0    0        0     0            0   \n",
       "\n",
       "   able get  ...  yukos said  yushchenko  zealand  zero  zhang  zimbabwe  \\\n",
       "0         0  ...           0           0        0     0      0         0   \n",
       "1         0  ...           0           0        0     0      0         0   \n",
       "2         0  ...           0           0        0     0      0         0   \n",
       "3         0  ...           0           0        0     0      0         0   \n",
       "4         0  ...           0           0        0     0      0         0   \n",
       "\n",
       "   zombie  zone  zurich  zurich premiership  \n",
       "0       0     0       0                   0  \n",
       "1       0     0       0                   0  \n",
       "2       0     0       0                   0  \n",
       "3       0     0       0                   0  \n",
       "4       0     0       0                   0  \n",
       "\n",
       "[5 rows x 10721 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_cv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(98.60523296687713, 98.6536934759663)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X_train_cv == 0).mean().mean() * 100, (X_test_cv == 0).mean().mean() * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tf_idf = pd.DataFrame(tf_idf.fit_transform(X_train).todense())\n",
    "X_train_tf_idf.columns = tf_idf.get_feature_names()\n",
    "\n",
    "X_test_tf_idf = pd.DataFrame(tf_idf.transform(X_test).todense())\n",
    "X_test_tf_idf.columns = tf_idf.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1668, 10721), (557, 10721))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tf_idf.shape, X_test_tf_idf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(98.60523296687713, 98.6536934759663)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X_train_tf_idf == 0).mean().mean() * 100, (X_test_tf_idf == 0).mean().mean() * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaa</th>\n",
       "      <th>abandoned</th>\n",
       "      <th>abc</th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>abroad</th>\n",
       "      <th>absence</th>\n",
       "      <th>absolute</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>abuse</th>\n",
       "      <th>...</th>\n",
       "      <th>young people</th>\n",
       "      <th>younger</th>\n",
       "      <th>youngest</th>\n",
       "      <th>youngster</th>\n",
       "      <th>youth</th>\n",
       "      <th>yuan</th>\n",
       "      <th>yukos</th>\n",
       "      <th>zealand</th>\n",
       "      <th>zero</th>\n",
       "      <th>zone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.043807</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 4965 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   aaa  abandoned  abc  ability  able    abroad  absence  absolute  \\\n",
       "0  0.0        0.0  0.0      0.0   0.0  0.043807      0.0       0.0   \n",
       "1  0.0        0.0  0.0      0.0   0.0  0.000000      0.0       0.0   \n",
       "2  0.0        0.0  0.0      0.0   0.0  0.000000      0.0       0.0   \n",
       "3  0.0        0.0  0.0      0.0   0.0  0.000000      0.0       0.0   \n",
       "4  0.0        0.0  0.0      0.0   0.0  0.000000      0.0       0.0   \n",
       "\n",
       "   absolutely  abuse  ...  young people  younger  youngest  youngster  youth  \\\n",
       "0         0.0    0.0  ...           0.0      0.0       0.0        0.0    0.0   \n",
       "1         0.0    0.0  ...           0.0      0.0       0.0        0.0    0.0   \n",
       "2         0.0    0.0  ...           0.0      0.0       0.0        0.0    0.0   \n",
       "3         0.0    0.0  ...           0.0      0.0       0.0        0.0    0.0   \n",
       "4         0.0    0.0  ...           0.0      0.0       0.0        0.0    0.0   \n",
       "\n",
       "   yuan  yukos  zealand  zero  zone  \n",
       "0   0.0    0.0      0.0   0.0   0.0  \n",
       "1   0.0    0.0      0.0   0.0   0.0  \n",
       "2   0.0    0.0      0.0   0.0   0.0  \n",
       "3   0.0    0.0      0.0   0.0   0.0  \n",
       "4   0.0    0.0      0.0   0.0   0.0  \n",
       "\n",
       "[5 rows x 4965 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tf_idf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_modelling_report(model, X_train, y_train, X_test, y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "    print('---------------------------------')\n",
    "    print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********************************************************************************\n",
      "Model:  MultinomialNB()\n",
      "\n",
      "\tUsing CountVectorizer........\n",
      "Confusion Matrix:\n",
      " [[121   2   3   0   2]\n",
      " [  1  93   1   0   2]\n",
      " [  0   0 104   0   0]\n",
      " [  0   0   0 128   0]\n",
      " [  1   0   0   0  99]]\n",
      "---------------------------------\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.95      0.96       128\n",
      "           1       0.98      0.96      0.97        97\n",
      "           2       0.96      1.00      0.98       104\n",
      "           3       1.00      1.00      1.00       128\n",
      "           4       0.96      0.99      0.98       100\n",
      "\n",
      "    accuracy                           0.98       557\n",
      "   macro avg       0.98      0.98      0.98       557\n",
      "weighted avg       0.98      0.98      0.98       557\n",
      "\n",
      "\n",
      "\tUsing TfidfVectorizer........\n",
      "Confusion Matrix:\n",
      " [[124   0   3   0   1]\n",
      " [  1  93   1   0   2]\n",
      " [  1   0 102   1   0]\n",
      " [  0   0   0 128   0]\n",
      " [  2   0   0   0  98]]\n",
      "---------------------------------\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97       128\n",
      "           1       1.00      0.96      0.98        97\n",
      "           2       0.96      0.98      0.97       104\n",
      "           3       0.99      1.00      1.00       128\n",
      "           4       0.97      0.98      0.98       100\n",
      "\n",
      "    accuracy                           0.98       557\n",
      "   macro avg       0.98      0.98      0.98       557\n",
      "weighted avg       0.98      0.98      0.98       557\n",
      "\n",
      "**********************************************************************************\n",
      "Model:  RandomForestClassifier()\n",
      "\n",
      "\tUsing CountVectorizer........\n",
      "Confusion Matrix:\n",
      " [[124   0   3   0   1]\n",
      " [  2  91   3   0   1]\n",
      " [  3   0  97   2   2]\n",
      " [  0   0   0 128   0]\n",
      " [  1   1   0   0  98]]\n",
      "---------------------------------\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.97      0.96       128\n",
      "           1       0.99      0.94      0.96        97\n",
      "           2       0.94      0.93      0.94       104\n",
      "           3       0.98      1.00      0.99       128\n",
      "           4       0.96      0.98      0.97       100\n",
      "\n",
      "    accuracy                           0.97       557\n",
      "   macro avg       0.97      0.96      0.96       557\n",
      "weighted avg       0.97      0.97      0.97       557\n",
      "\n",
      "\n",
      "\tUsing TfidfVectorizer........\n",
      "Confusion Matrix:\n",
      " [[123   0   4   0   1]\n",
      " [  3  92   1   0   1]\n",
      " [  1   0  98   3   2]\n",
      " [  1   0   0 127   0]\n",
      " [  3   1   0   1  95]]\n",
      "---------------------------------\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.96      0.95       128\n",
      "           1       0.99      0.95      0.97        97\n",
      "           2       0.95      0.94      0.95       104\n",
      "           3       0.97      0.99      0.98       128\n",
      "           4       0.96      0.95      0.95       100\n",
      "\n",
      "    accuracy                           0.96       557\n",
      "   macro avg       0.96      0.96      0.96       557\n",
      "weighted avg       0.96      0.96      0.96       557\n",
      "\n",
      "**********************************************************************************\n",
      "Model:  DecisionTreeClassifier()\n",
      "\n",
      "\tUsing CountVectorizer........\n",
      "Confusion Matrix:\n",
      " [[104   2  11   4   7]\n",
      " [  4  84   1   6   2]\n",
      " [  6   3  85   6   4]\n",
      " [  1   2   0 123   2]\n",
      " [  5   3   2   3  87]]\n",
      "---------------------------------\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.81      0.84       128\n",
      "           1       0.89      0.87      0.88        97\n",
      "           2       0.86      0.82      0.84       104\n",
      "           3       0.87      0.96      0.91       128\n",
      "           4       0.85      0.87      0.86       100\n",
      "\n",
      "    accuracy                           0.87       557\n",
      "   macro avg       0.87      0.87      0.87       557\n",
      "weighted avg       0.87      0.87      0.87       557\n",
      "\n",
      "\n",
      "\tUsing TfidfVectorizer........\n",
      "Confusion Matrix:\n",
      " [[102   3  12   6   5]\n",
      " [  3  84   2   6   2]\n",
      " [  4   3  88   5   4]\n",
      " [  1   2   1 122   2]\n",
      " [  6   4   2   2  86]]\n",
      "---------------------------------\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.80      0.84       128\n",
      "           1       0.88      0.87      0.87        97\n",
      "           2       0.84      0.85      0.84       104\n",
      "           3       0.87      0.95      0.91       128\n",
      "           4       0.87      0.86      0.86       100\n",
      "\n",
      "    accuracy                           0.87       557\n",
      "   macro avg       0.87      0.86      0.86       557\n",
      "weighted avg       0.87      0.87      0.86       557\n",
      "\n",
      "**********************************************************************************\n",
      "Model:  KNeighborsClassifier()\n",
      "\n",
      "\tUsing CountVectorizer........\n",
      "Confusion Matrix:\n",
      " [[ 74   0   1  52   1]\n",
      " [  2  46   0  49   0]\n",
      " [  2   0  67  35   0]\n",
      " [  0   0   0 128   0]\n",
      " [  5   4   0  59  32]]\n",
      "---------------------------------\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.58      0.70       128\n",
      "           1       0.92      0.47      0.63        97\n",
      "           2       0.99      0.64      0.78       104\n",
      "           3       0.40      1.00      0.57       128\n",
      "           4       0.97      0.32      0.48       100\n",
      "\n",
      "    accuracy                           0.62       557\n",
      "   macro avg       0.83      0.60      0.63       557\n",
      "weighted avg       0.81      0.62      0.63       557\n",
      "\n",
      "\n",
      "\tUsing TfidfVectorizer........\n",
      "Confusion Matrix:\n",
      " [[119   0   5   1   3]\n",
      " [  1  91   1   0   4]\n",
      " [  2   2  99   1   0]\n",
      " [  2   0   0 126   0]\n",
      " [  2   1   3   0  94]]\n",
      "---------------------------------\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.93      0.94       128\n",
      "           1       0.97      0.94      0.95        97\n",
      "           2       0.92      0.95      0.93       104\n",
      "           3       0.98      0.98      0.98       128\n",
      "           4       0.93      0.94      0.94       100\n",
      "\n",
      "    accuracy                           0.95       557\n",
      "   macro avg       0.95      0.95      0.95       557\n",
      "weighted avg       0.95      0.95      0.95       557\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model in [MultinomialNB(), RandomForestClassifier(), DecisionTreeClassifier(), KNeighborsClassifier()]:\n",
    "    print('*****************************************'*2)\n",
    "    print('Model: ', model)\n",
    "    print('\\n\\tUsing CountVectorizer........')\n",
    "    generate_modelling_report(model, X_train_cv, y_train, X_test_cv, y_test)\n",
    "    print('\\n\\tUsing TfidfVectorizer........')\n",
    "    generate_modelling_report(model, X_train_tf_idf, y_train, X_test_tf_idf, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"**********************************************************************************\n",
    "Model:  MultinomialNB()\n",
    "\n",
    "\tUsing CountVectorizer........\n",
    "Confusion Matrix:\n",
    " [[121   2   3   0   2]\n",
    " [  1  93   1   0   2]\n",
    " [  0   0 104   0   0]\n",
    " [  0   0   0 128   0]\n",
    " [  1   0   0   0  99]]\n",
    "---------------------------------\n",
    "Classification Report:\n",
    "               precision    recall  f1-score   support\n",
    "\n",
    "           0       0.98      0.95      0.96       128\n",
    "           1       0.98      0.96      0.97        97\n",
    "           2       0.96      1.00      0.98       104\n",
    "           3       1.00      1.00      1.00       128\n",
    "           4       0.96      0.99      0.98       100\n",
    "\n",
    "    accuracy                           0.98       557\n",
    "   macro avg       0.98      0.98      0.98       557\n",
    "weighted avg       0.98      0.98      0.98       557\n",
    "\n",
    "\n",
    "\tUsing TfidfVectorizer........\n",
    "Confusion Matrix:\n",
    " [[124   0   3   0   1]\n",
    " [  1  93   1   0   2]\n",
    " [  1   0 102   1   0]\n",
    " [  0   0   0 128   0]\n",
    " [  2   0   0   0  98]]\n",
    "---------------------------------\n",
    "Classification Report:\n",
    "               precision    recall  f1-score   support\n",
    "\n",
    "           0       0.97      0.97      0.97       128\n",
    "           1       1.00      0.96      0.98        97\n",
    "           2       0.96      0.98      0.97       104\n",
    "           3       0.99      1.00      1.00       128\n",
    "           4       0.97      0.98      0.98       100\n",
    "\n",
    "    accuracy                           0.98       557\n",
    "   macro avg       0.98      0.98      0.98       557\n",
    "weighted avg       0.98      0.98      0.98       557\n",
    "\n",
    "**********************************************************************************\n",
    "Model:  RandomForestClassifier()\n",
    "\n",
    "\tUsing CountVectorizer........\n",
    "Confusion Matrix:\n",
    " [[124   0   3   0   1]\n",
    " [  2  91   3   0   1]\n",
    " [  3   0  97   2   2]\n",
    " [  0   0   0 128   0]\n",
    " [  1   1   0   0  98]]\n",
    "---------------------------------\n",
    "Classification Report:\n",
    "               precision    recall  f1-score   support\n",
    "\n",
    "           0       0.95      0.97      0.96       128\n",
    "           1       0.99      0.94      0.96        97\n",
    "           2       0.94      0.93      0.94       104\n",
    "           3       0.98      1.00      0.99       128\n",
    "           4       0.96      0.98      0.97       100\n",
    "\n",
    "    accuracy                           0.97       557\n",
    "   macro avg       0.97      0.96      0.96       557\n",
    "weighted avg       0.97      0.97      0.97       557\n",
    "\n",
    "\n",
    "\tUsing TfidfVectorizer........\n",
    "Confusion Matrix:\n",
    " [[123   0   4   0   1]\n",
    " [  3  92   1   0   1]\n",
    " [  1   0  98   3   2]\n",
    " [  1   0   0 127   0]\n",
    " [  3   1   0   1  95]]\n",
    "---------------------------------\n",
    "Classification Report:\n",
    "               precision    recall  f1-score   support\n",
    "\n",
    "           0       0.94      0.96      0.95       128\n",
    "           1       0.99      0.95      0.97        97\n",
    "           2       0.95      0.94      0.95       104\n",
    "           3       0.97      0.99      0.98       128\n",
    "           4       0.96      0.95      0.95       100\n",
    "\n",
    "    accuracy                           0.96       557\n",
    "   macro avg       0.96      0.96      0.96       557\n",
    "weighted avg       0.96      0.96      0.96       557\n",
    "\n",
    "**********************************************************************************\n",
    "Model:  DecisionTreeClassifier()\n",
    "\n",
    "\tUsing CountVectorizer........\n",
    "Confusion Matrix:\n",
    " [[104   2  11   4   7]\n",
    " [  4  84   1   6   2]\n",
    " [  6   3  85   6   4]\n",
    " [  1   2   0 123   2]\n",
    " [  5   3   2   3  87]]\n",
    "---------------------------------\n",
    "Classification Report:\n",
    "               precision    recall  f1-score   support\n",
    "\n",
    "           0       0.87      0.81      0.84       128\n",
    "           1       0.89      0.87      0.88        97\n",
    "           2       0.86      0.82      0.84       104\n",
    "           3       0.87      0.96      0.91       128\n",
    "           4       0.85      0.87      0.86       100\n",
    "\n",
    "    accuracy                           0.87       557\n",
    "   macro avg       0.87      0.87      0.87       557\n",
    "weighted avg       0.87      0.87      0.87       557\n",
    "\n",
    "\n",
    "\tUsing TfidfVectorizer........\n",
    "Confusion Matrix:\n",
    " [[102   3  12   6   5]\n",
    " [  3  84   2   6   2]\n",
    " [  4   3  88   5   4]\n",
    " [  1   2   1 122   2]\n",
    " [  6   4   2   2  86]]\n",
    "---------------------------------\n",
    "Classification Report:\n",
    "               precision    recall  f1-score   support\n",
    "\n",
    "           0       0.88      0.80      0.84       128\n",
    "           1       0.88      0.87      0.87        97\n",
    "           2       0.84      0.85      0.84       104\n",
    "           3       0.87      0.95      0.91       128\n",
    "           4       0.87      0.86      0.86       100\n",
    "\n",
    "    accuracy                           0.87       557\n",
    "   macro avg       0.87      0.86      0.86       557\n",
    "weighted avg       0.87      0.87      0.86       557\n",
    "\n",
    "**********************************************************************************\n",
    "Model:  KNeighborsClassifier()\n",
    "\n",
    "\tUsing CountVectorizer........\n",
    "Confusion Matrix:\n",
    " [[ 74   0   1  52   1]\n",
    " [  2  46   0  49   0]\n",
    " [  2   0  67  35   0]\n",
    " [  0   0   0 128   0]\n",
    " [  5   4   0  59  32]]\n",
    "---------------------------------\n",
    "Classification Report:\n",
    "               precision    recall  f1-score   support\n",
    "\n",
    "           0       0.89      0.58      0.70       128\n",
    "           1       0.92      0.47      0.63        97\n",
    "           2       0.99      0.64      0.78       104\n",
    "           3       0.40      1.00      0.57       128\n",
    "           4       0.97      0.32      0.48       100\n",
    "\n",
    "    accuracy                           0.62       557\n",
    "   macro avg       0.83      0.60      0.63       557\n",
    "weighted avg       0.81      0.62      0.63       557\n",
    "\n",
    "\n",
    "\tUsing TfidfVectorizer........\n",
    "Confusion Matrix:\n",
    " [[119   0   5   1   3]\n",
    " [  1  91   1   0   4]\n",
    " [  2   2  99   1   0]\n",
    " [  2   0   0 126   0]\n",
    " [  2   1   3   0  94]]\n",
    "---------------------------------\n",
    "Classification Report:\n",
    "               precision    recall  f1-score   support\n",
    "\n",
    "           0       0.94      0.93      0.94       128\n",
    "           1       0.97      0.94      0.95        97\n",
    "           2       0.92      0.95      0.93       104\n",
    "           3       0.98      0.98      0.98       128\n",
    "           4       0.93      0.94      0.94       100\n",
    "\n",
    "    accuracy                           0.95       557\n",
    "   macro avg       0.95      0.95      0.95       557\n",
    "weighted avg       0.95      0.95      0.95       557\n",
    "\n",
    "\"\"\"\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questionnaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many news articles are present in the dataset that we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2225"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most of the news articles are from _____ category.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sports'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Category'].value_counts().idxmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only ___ no. of articles belong to the ‘Technology’ category.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "401"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df['Category'] == 'Technology').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are Stop Words and why should they be removed from the text data?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop words are common, uninformative words like \"the,\" \"and,\" and \"in.\" They're removed from text data because they don't contribute much to meaning and can slow down processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain the difference between Stemming and Lemmatization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both normalize words, but stemming shortens words to their base, even if it's not a real word (\"chang\" for \"changes\"). Lemmatization gets real word bases (\"run\" for \"running\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which of the techniques Bag of Words or TF-IDF is considered to be more efficient than the other?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of Words (BoW): Counts words in documents equally.\n",
    "TF-IDF: Considers word importance; slower due to IDF calculation.\n",
    "In practice, TF-IDF's slight efficiency difference isn't usually significant, and its weighting makes it more versatile for NLP tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What’s the shape of train & test data sets after performing a 75:25 split.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1668, 557)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape[0], y_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which of the following is found to be the best performing model..\n",
    "\n",
    "# a. Random Forest b. Nearest Neighbors c. Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# According to this particular use case, both precision and recall are equally important. (T/F)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "True"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scaler",
   "language": "python",
   "name": "scaler"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
